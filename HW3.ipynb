{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from judicial_splitter import splitter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors, Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim import matutils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(text, mode):\n",
    "    morph = MorphAnalyzer()\n",
    "    stops = stopwords.words('russian')\n",
    "    punct = string.punctuation + '«»“”—…'\n",
    "    \n",
    "    text = text.lower()\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    all_lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    \n",
    "    if mode == 'd2v':\n",
    "        cleaned_lemmas = [lemma for lemma in all_lemmas if lemma not in punct]\n",
    "    else:\n",
    "        cleaned_lemmas = [lemma for lemma in all_lemmas if lemma not in stops and lemma not in punct]\n",
    " \n",
    "    return cleaned_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = os.listdir('article')[:10]\n",
    "corpus = ['article/' + file for file in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_and_stats(corpus):\n",
    "    prep_corpus = []\n",
    "    doc_lengths = {}\n",
    "\n",
    "    for filename in corpus:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            file = file.read()\n",
    "\n",
    "        text = preprocessing(file, 'ii')\n",
    "        doc_lengths[filename] = len(text)\n",
    "\n",
    "        prep_corpus.append(' '.join(text))\n",
    "    \n",
    "    avgdl = sum(doc_lengths.values()) / len(doc_lengths)\n",
    "    \n",
    "    return prep_corpus, doc_lengths, avgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_idfs(term_doc_matrix):    \n",
    "    idfs = {}\n",
    "\n",
    "    docs = term_doc_matrix[0]\n",
    "    words = term_doc_matrix[1]\n",
    "    numbers = term_doc_matrix[2]\n",
    "\n",
    "    N = len(docs)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        n = 0\n",
    "\n",
    "        for num in numbers:\n",
    "            val = num[i]\n",
    "\n",
    "            if val != 0:\n",
    "                n += 1\n",
    "\n",
    "        idf = log((N - n + 0.5) / (n + 0.5))\n",
    "        idfs[word] = idf\n",
    "    \n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverted_index(corpus):\n",
    "    prep_corpus, doc_lengths, avgdl = corpus_and_stats(corpus)\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    docs = vectorizer.fit_transform(prep_corpus)\n",
    "\n",
    "    term_doc_matrix = pd.DataFrame(docs.toarray(), columns=vectorizer.get_feature_names(), index=corpus)\n",
    "    term_doc_matrix = [term_doc_matrix.index.tolist(), term_doc_matrix.columns.tolist(), term_doc_matrix.values.tolist()]\n",
    "    \n",
    "    idfs = calc_idfs(term_doc_matrix)\n",
    "    \n",
    "    return term_doc_matrix, doc_lengths, avgdl, idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(paragraph):\n",
    "    counter = 0\n",
    "    hypervector = np.zeros(300)\n",
    "    stops = stopwords.words('russian')\n",
    "    \n",
    "    for word in paragraph:\n",
    "        try:\n",
    "            vector = np.array(model.wv[word])\n",
    "            hypervector += vector\n",
    "            counter += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    hypervector = hypervector / counter\n",
    "    \n",
    "    return hypervector\n",
    "\n",
    "\n",
    "def save_w2v_base(corpus):\n",
    "    base = []\n",
    "\n",
    "    for filename in corpus:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            file = file.read()\n",
    "\n",
    "        paragraphs = splitter(file, 4)\n",
    "\n",
    "        for par in paragraphs:\n",
    "            prep_par = preprocessing(par, 'w2v')\n",
    "            vector = get_w2v_vectors(prep_par)\n",
    "            base.append([vector.tolist(), filename])\n",
    "            \n",
    "    with open('jud_base_w2v.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(base, file, allow_nan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class D2VIterator(object):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i, text in enumerate(self.texts):\n",
    "              yield LabeledSentence(text, [self.labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(d2v_iterator, model_name):\n",
    "    model = Doc2Vec(vector_size=300, min_count=0, alpha=0.025, min_alpha=0.025)\n",
    "    model.build_vocab(d2v_iterator)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train(d2v_iterator, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        model.alpha -= 0.002\n",
    "        model.min_alpha = model.alpha\n",
    "        model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(corpus):\n",
    "    prep_corpus, labels = [], []\n",
    "\n",
    "    for filename in corpus:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            file = file.read()\n",
    "\n",
    "        paragraphs = splitter(file, 4)\n",
    "\n",
    "        for i, par in enumerate(paragraphs):\n",
    "            prep_par = preprocessing(par, 'd2v')\n",
    "            prep_corpus.append(' '.join(text))\n",
    "            labels.append(filename)\n",
    "        \n",
    "    d2v_iterator = D2VIterator(prep_corpus, labels)\n",
    "    train_doc2vec(d2v_iterator, 'doc2vec.model')\n",
    "    trained_model = Doc2Vec.load('doc2vec.model')\n",
    "    \n",
    "    return trained_model, labels\n",
    "\n",
    "\n",
    "def save_d2v_base(corpus):\n",
    "    trained_model, labels = get_d2v_vectors(corpus)\n",
    "    base = []\n",
    "    \n",
    "    for label in labels:\n",
    "        vector = trained_model.docvecs[label]\n",
    "        base.append([vector.tolist(), label])\n",
    "        \n",
    "    with open('jud_base_d2v.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(base, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прогоняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inverted index\n",
    "term_doc_matrix, doc_lengths, avgdl, idfs = inverted_index(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec\n",
    "save_w2v_base(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc2vec\n",
    "save_d2v_base(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "def score_BM25(idf, freq, D, avgdl) -> float:\n",
    "    score = idf * (k1 + 1) * freq / (freq + k1 * (1 - b + b * D / avgdl))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    \n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v(request):\n",
    "    vec_request = get_w2v_vectors(request)    \n",
    "    results = {}\n",
    "    \n",
    "    with open('jud_base_w2v.json', 'r', encoding='utf-8') as file:\n",
    "        base = json.load(file)\n",
    "    \n",
    "    for elem in base:\n",
    "        res = similarity(vec_request, elem[0])\n",
    "        results[elem[1]] = res\n",
    "    \n",
    "    return sorted(results.items(), key=lambda kv: kv[1], reverse=False)\n",
    "      \n",
    "    \n",
    "def search_d2v(request):\n",
    "    vec_request = trained_model.infer_vector(request)    \n",
    "    results = {}\n",
    "    \n",
    "    with open('jud_base_d2v.json', 'r', encoding='utf-8') as file:\n",
    "        base = json.load(file)\n",
    "    \n",
    "    for elem in base:\n",
    "        res = similarity(vec_request, elem[0])\n",
    "        results[elem[1]] = res    \n",
    "    \n",
    "    return sorted(results.items(), key=lambda kv: kv[1], reverse=False)\n",
    "\n",
    "\n",
    "def search_inv_index(request):\n",
    "    results = {doc: 0 for doc in term_doc_matrix[0]}\n",
    "    \n",
    "    for word in request:\n",
    "        if word in term_doc_matrix[1]:\n",
    "            word_ind = term_doc_matrix[1].index(word)\n",
    "            \n",
    "            for i, doc in enumerate(term_doc_matrix[0]):\n",
    "                freq = term_doc_matrix[2][i][word_ind]\n",
    "                D = doc_lengths[doc]\n",
    "                idf = idfs[word]      \n",
    "                bm25 = score_BM25(idf, freq, D, avgdl)\n",
    "                \n",
    "                results[doc] += bm25\n",
    "    \n",
    "    return sorted(results.items(), key=lambda kv: kv[1], reverse=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения всех этих заданий ваш поисковик готов, поздравляю!                  \n",
    "Осталось завернуть все написанное в питон скрипт, и сделать общую функцию поиска гибким, чтобы мы могли искать как по обратному индексу, так и по word2vec, так и по doc2vec.          \n",
    "Сделать это можно очень просто через старый добрый ``` if ```, который будет дергать ту или иную функцию поиска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(request, search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        request = preprocessing(request, 'ii')\n",
    "        search_result = search_inv_index(request)\n",
    "        \n",
    "    elif search_method == 'word2vec':\n",
    "        request = preprocessing(request, 'w2v')\n",
    "        search_result = search_w2v(request)\n",
    "        \n",
    "    elif search_method == 'doc2vec':\n",
    "        request = ' '.join(preprocessing(request, 'd2v'))\n",
    "        search_result = search_d2v(request)\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    \n",
    "    return search_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = search('утка охота соболь лицензия ружье', 'inverted_index')\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = search('утка охота соболь лицензия ружье', 'word2vec')\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = search('утка охота соболь лицензия ружье', 'doc2vec')\n",
    "results[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
